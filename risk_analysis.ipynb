{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python for Credit Card Default Risk: Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I will use random and SMOTE oversampling in combination with logistic regression to predict whether or not someone is likely to default on their credit card loans in a given month using demographic information. Like fraud, defaults are more of the exception than the norm. Because they're underrepresented in the dataset, it can be useful to oversample defaults and balance the classes. If that all sounds too complicated -- don't worry. It's way simpler than you think!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic imports\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>ln_balance_limit</th>\n",
       "      <th>sex</th>\n",
       "      <th>education</th>\n",
       "      <th>marriage</th>\n",
       "      <th>age</th>\n",
       "      <th>default_next_month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>9.903488</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>11.695247</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>11.407565</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>10.819778</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>10.819778</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  ln_balance_limit  sex  education  marriage  age  default_next_month\n",
       "0   1          9.903488    1          2         0   24                   1\n",
       "1   2         11.695247    1          2         1   26                   1\n",
       "2   3         11.407565    1          2         1   34                   0\n",
       "3   4         10.819778    1          2         0   37                   0\n",
       "4   5         10.819778    0          2         0   57                   0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(30000, 7)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# First we'll bring in the data using Path and Pandas\n",
    "data_path = Path('data/cc_default.csv')\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Here is how our data looks:\n",
    "display(df.head())\n",
    "display(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 7 columns (although the ID column isn't a feature we're interested in). And we have 30,000 rows -- not very large but good enough for this example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we need to seperate our features from the target we're trying to predict.\n",
    "feature_cols = [i for i in df.columns if i not in (\"ID\", \"default_next_month\")]\n",
    "X = df[feature_cols]\n",
    "y = df['default_next_month']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There were 6636 defaults out of 30000 customers, a rate of 22.12%\n"
     ]
    }
   ],
   "source": [
    "# Let's see what the default rate looks like. I'll make a quick function to do this so we can use it later\n",
    "def view_target_pop(target) -> None:\n",
    "    \"\"\"A function that prints out the positive and negative counts in our target column, e.g. the y column\"\"\"\n",
    "    vals = list(Counter(target).values())\n",
    "    print(f\"There were {vals[0]} defaults out of 30000 customers, a rate of {100*(vals[0]/30000):.2f}%\")\n",
    "    return None\n",
    "\n",
    "view_target_pop(y)    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That seems like a high rate! Perhaps we had already identified this set as \"at risk\". Either way, we still have an **imbalanced class**: there are around 4.5 times more non-defaults than defaults. So we'll try out some cool Python libraries to help us make better predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we're going to use Python's sklearn module to split our data into training and testing sets.\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There were 17532 defaults out of 30000 customers, a rate of 58.44%\n"
     ]
    }
   ],
   "source": [
    "# Next we're going to import a Random Over Sampling model from the imblearn library. \n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "ros = RandomOverSampler(random_state=1)\n",
    "X_resampled, y_resampled = ros.fit_resample(X_train, y_train)\n",
    "\n",
    "view_target_pop(y_resampled)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! No we have closer to a 50-50 split. Let's try some basic logistic regression and see how we do at predicting defaults."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(random_state=1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression(solver= 'lbfgs', random_state=1)\n",
    "model.fit(X_resampled, y_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3744, 2088],\n",
       "       [ 745,  923]], dtype=int64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred = model.predict(X_test)\n",
    "confusion_matrix(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In my experience, confusion matrices can be just that -- *confusing*! But it's pretty simple. In binary classification (read -- is it a default or not?) We want to be specific about ***how*** we're right and ***how*** we're wrong. So let's go a little further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overall accuracy: 60.0%\n",
      "----------------------------------------------------------------------------------\n",
      "                   pre       rec       spe        f1       geo       iba       sup\n",
      "\n",
      "          0       0.83      0.64      0.55      0.73      0.60      0.36      5832\n",
      "          1       0.31      0.55      0.64      0.39      0.60      0.35      1668\n",
      "\n",
      "avg / total       0.72      0.62      0.57      0.65      0.60      0.36      7500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# I'm going to bring in some more modules from the sklearn and imblearn libraries to help breakdown our model's performance so far.\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from imblearn.metrics import classification_report_imbalanced\n",
    "\n",
    "print(f\"overall accuracy: {100*round(balanced_accuracy_score(y_test, y_pred),2)}%\")\n",
    "print(\"----------------------------------------------------------------------------------\")\n",
    "print(classification_report_imbalanced(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So an overall accuracy of 60% might be good or bad -- depends on the use case. But when we break into the classification report, we can see what the model is good at and where it falls off. \n",
    "- The model has *high precision* when classifying a borrower as a *non-default* (0). Precision gives us an idea about the **meaningfulness** of a positive prediction, and is equal to the number of true negatives (3744) divided by the total number of true and false negatives (3744+745). \n",
    "- However the model has *low precision* when detecting defaults. Let's see if we can do better with SMOTEEN (Synthetic Minority Oversampling Technique), which is a form of combination sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There were 8179 defaults out of 30000 customers, a rate of 27.26%\n"
     ]
    }
   ],
   "source": [
    "# Like before, we're going to import the SMOTE model and fit it to the data then check the count of each class\n",
    "from imblearn.combine import SMOTEENN\n",
    "\n",
    "smote_enn = SMOTEENN(random_state=0)\n",
    "X_resampled, y_resampled = smote_enn.fit_resample(\n",
    "    X_train, y_train\n",
    ")\n",
    "from collections import Counter\n",
    "\n",
    "view_target_pop(y_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4686 1146]\n",
      " [1094  574]]\n",
      "----------------------------------------------------------------------------------\n",
      "overall accuracy: 56.99999999999999%\n",
      "----------------------------------------------------------------------------------\n",
      "                   pre       rec       spe        f1       geo       iba       sup\n",
      "\n",
      "          0       0.81      0.80      0.34      0.81      0.53      0.29      5832\n",
      "          1       0.33      0.34      0.80      0.34      0.53      0.26      1668\n",
      "\n",
      "avg / total       0.70      0.70      0.45      0.70      0.53      0.28      7500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's rerun our logistic regression and see if we do any better\n",
    "model = LogisticRegression(solver= 'lbfgs', random_state=1)\n",
    "model.fit(X_resampled, y_resampled)\n",
    "y_pred = model.predict(X_test)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(\"----------------------------------------------------------------------------------\")\n",
    "print(f\"overall accuracy: {100*round(balanced_accuracy_score(y_test, y_pred),2)}%\")\n",
    "print(\"----------------------------------------------------------------------------------\")\n",
    "print(classification_report_imbalanced(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we marginally declined in overall accuracy, however we can now better predict non-defaults (F1 score of 0.81 compared to 0.73), and we know that when our model predicts a customer will default, there is a "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this balanced class, I'll do some more advanced modeling and see if we can do a better job at predicting defaults. Here I'll used Random Forest and compare the results to Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BalancedRandomForestClassifier(n_estimators=1000, random_state=1)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "brf = BalancedRandomForestClassifier(n_estimators=1000, random_state=1)\n",
    "brf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3130 2702]\n",
      " [ 683  985]]\n",
      "----------------------------------------------------------------------------------\n",
      "overall accuracy: 56.00000000000001%\n",
      "----------------------------------------------------------------------------------\n",
      "                   pre       rec       spe        f1       geo       iba       sup\n",
      "\n",
      "          0       0.82      0.54      0.59      0.65      0.56      0.32      5832\n",
      "          1       0.27      0.59      0.54      0.37      0.56      0.32      1668\n",
      "\n",
      "avg / total       0.70      0.55      0.58      0.59      0.56      0.32      7500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Now I'll print out the imbalanced classification report for our balanced random forest classifier\n",
    "y_pred_rf = brf.predict(X_test)\n",
    "print(confusion_matrix(y_test, y_pred_rf))\n",
    "print(\"----------------------------------------------------------------------------------\")\n",
    "print(f\"overall accuracy: {100*round(balanced_accuracy_score(y_test, y_pred_rf),2)}%\")\n",
    "print(\"----------------------------------------------------------------------------------\")\n",
    "print(classification_report_imbalanced(y_test, y_pred_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So with the balanced forest, we get much better **recall** when detecting defaults. In medical science, recall is sometimes called **sensitivity**, because it calculates how many of the Actual Positives our model captures. So how would you decide which model to use? When the cost of a *false negative* is higher than the cost of a *false positive*, the model with better **RECALL** is the right choice. \n",
    "\n",
    "From a business perspective, a false positive means we label someone as a default when they, in fact, make their payment. Perhaps this leads to over-hedging our portfolio, which could be costly. However, a false negative seems worse on the face of it. If we label someone as a non-default and they actually default, then we're susceptible to unforseen risk. \n",
    "\n",
    "Hopefully this was a helpful break down of some basic statistical packages in Python that lenders can use to build predicitve models!\n",
    "\n",
    "-- Created by Laramie Dunlap on 9/21/2022"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.11 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "83e1771ce1f9df5de1670795847cb857ef6bf5f13021e3cce81b976772d7099f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
